# -*- coding: utf-8 -*-
"""Predict_Employee_Attrition_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16nFBhzZLHQ6HJHHUl-4KDni7ozMeSRdb
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

data = pd.read_csv("/content/WA_Fn-UseC_-HR-Employee-Attrition[1].csv")

data.head()

data.info()

data.describe()

data.isnull().sum()

data.columns

data.head()

data["Attrition"].value_counts()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8,5))
sns.histplot(data['Age'], bins=30, kde=True, color='skyblue')
plt.title('Age Distribution')
plt.xlabel('Age')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(8,5))
sns.countplot(data=data, x='BusinessTravel', hue='Attrition')
plt.title('Attrition by Business Travel')
plt.xlabel('Business Travel')
plt.ylabel('Count')
plt.legend(title='Attrition', labels=['No', 'Yes'])
plt.show()

plt.figure(figsize=(8,5))
sns.boxplot(data=data, x='Attrition', y='MonthlyIncome')
plt.title('Monthly Income by Attrition')
plt.xlabel('Attrition (0=No, 1=Yes)')
plt.ylabel('Monthly Income')
plt.show()

plt.figure(figsize=(10,6))
job_role_counts = data['JobRole'].value_counts()
sns.barplot(y=job_role_counts.index, x=job_role_counts.values, palette='muted')
plt.title('Count of Employees by Job Role')
plt.xlabel('Count')
plt.ylabel('Job Role')
plt.show()

plt.figure(figsize=(8,5))
sns.boxplot(data=data, x='Attrition', y='DistanceFromHome')
plt.title('Distance From Home by Attrition')
plt.xlabel('Attrition (0=No, 1=Yes)')
plt.ylabel('Distance From Home (miles)')
plt.show()

plt.figure(figsize=(8,5))
sns.histplot(data['YearsAtCompany'], bins=30, kde=False, color='salmon')
plt.title('Years At Company Distribution')
plt.xlabel('Years At Company')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(8,5))
sns.countplot(data=data, x='OverTime', hue='Attrition')
plt.title('Attrition by OverTime')
plt.xlabel('OverTime')
plt.ylabel('Count')
plt.legend(title='Attrition', labels=['No', 'Yes'])
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

top_features = ['PercentSalaryHike', 'YearsSinceLastPromotion', 'EnvironmentSatisfaction']

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for i, feature in enumerate(top_features):
    ax = axes[i]
    if data[feature].dtype == 'object' or len(data[feature].unique()) < 10:
        sns.countplot(data=data, x=feature, hue='Attrition', ax=ax)
        ax.set_title(f'Attrition vs {feature}')
        ax.legend(title='Attrition', labels=['No', 'Yes'])
    else:
        sns.boxplot(data=data, x='Attrition', y=feature, ax=ax)
        ax.set_title(f'Attrition vs {feature}')
        ax.set_xlabel('Attrition (0=No, 1=Yes)')

plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline
import warnings
import joblib

warnings.filterwarnings('ignore')

# === 1. Load the dataset ===
data = pd.read_csv("/content/WA_Fn-UseC_-HR-Employee-Attrition[1].csv")

# === 2. Encode all categorical columns ===
# Get all columns with object dtype (categorical columns)
categorical_cols = data.select_dtypes(include=['object']).columns.tolist()

# Convert target Attrition from Yes/No to 1/0
data['Attrition'] = data['Attrition'].apply(lambda x: 1 if x == 'Yes' else 0)

# Remove target from categorical columns if present
if 'Attrition' in categorical_cols:
    categorical_cols.remove('Attrition')

# Label encode all other categorical columns
for col in categorical_cols:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])

# === 3. Prepare features and target ===
X = data.drop("Attrition", axis=1)
y = data["Attrition"]

# === 4. Train/test split ===
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# === 5. Define models and hyperparameters ===
models = {
    "LogisticRegression": {
        "model": LogisticRegression(solver='liblinear', random_state=42),
        "params": {
            "model__C": [0.01, 0.1, 1, 10],
            "model__penalty": ['l1', 'l2'],
            "model__class_weight": ['balanced'],
        }
    },
    "DecisionTree": {
        "model": DecisionTreeClassifier(random_state=42),
        "params": {
            "model__max_depth": [3, 5, 10, None],
            "model__min_samples_split": [2, 5, 10],
            "model__class_weight": ['balanced']
        }
    },
    "RandomForest": {
        "model": RandomForestClassifier(random_state=42),
        "params": {
            "model__n_estimators": [50, 100],
            "model__max_depth": [None, 10, 20],
            "model__min_samples_split": [2, 5],
            "model__class_weight": ['balanced']
        }
    }
}

# Optional: Add XGBoost
try:
    from xgboost import XGBClassifier
    models["XGBoost"] = {
        "model": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),
        "params": {
            "model__learning_rate": [0.01, 0.1],
            "model__max_depth": [3, 5, 7],
            "model__n_estimators": [100, 200],
            "model__scale_pos_weight": [1, 2, 3]
        }
    }
except ImportError:
    print("XGBoost not installed. Skipping XGBoost model.")

# === 6. Train and Evaluate ===
best_model = None
best_recall_class1 = -1
best_model_name = ""
model_scores = {}

for name, mp in models.items():
    print(f"\n=== {name} ===")

    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('smote', SMOTE(random_state=42)),
        ('model', mp['model'])
    ])

    grid = GridSearchCV(pipeline, mp['params'], cv=3, scoring='f1', n_jobs=-1)
    grid.fit(X_train, y_train)

    y_pred = grid.predict(X_test)

    print("Best Params:", grid.best_params_)
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

    report_dict = classification_report(y_test, y_pred, digits=4, output_dict=True)
    print("\nClassification Report:\n", classification_report(y_test, y_pred, digits=4))

    f1_class1 = report_dict['1']['f1-score']
    recall_class1 = report_dict['1']['recall']
    f1_class0 = report_dict['0']['f1-score']

    model_scores[name] = {
        "f1_class1": f1_class1,
        "recall_class1": recall_class1,
        "precision_class1": report_dict['1']['precision'],
        "f1_class0": f1_class0,
        "recall_class0": report_dict['0']['recall'],
        "precision_class0": report_dict['0']['precision'],
        "accuracy": report_dict['accuracy']
    }

    # Save best model based on recall of class 1 (attrition)
    if recall_class1 > best_recall_class1:
        best_recall_class1 = recall_class1
        best_model = grid.best_estimator_
        best_model_name = name

# === 7. Save the best model ===
if best_model:
    joblib.dump(best_model, "best_attrition_model.pkl")
    print(f"\nâœ… Saved the best model ({best_model_name}) based on Recall of class '1' (Attrition) as 'best_attrition_model.pkl'")

# === 8. Summary of all models ===
summary_df = pd.DataFrame(model_scores).T.sort_values(by="recall_class1", ascending=False)

print("\n=== Model Performance Summary (based on class '1' Recall) ===")
print(summary_df[['recall_class1', 'f1_class1', 'precision_class1', 'accuracy']])

print("\n=== Model Performance Summary (based on class '0' F1-score) ===")
print(summary_df[['f1_class0', 'recall_class0', 'precision_class0']])

data.tail()

import pandas as pd
import numpy as np
import joblib

model = joblib.load("best_attrition_model.pkl")

columns = [
    'Age', 'BusinessTravel', 'DailyRate', 'Department', 'DistanceFromHome',
    'Education', 'EducationField', 'EmployeeCount', 'EmployeeNumber', 'EnvironmentSatisfaction',
    'Gender', 'HourlyRate', 'JobInvolvement', 'JobLevel', 'JobRole',
    'JobSatisfaction', 'MaritalStatus', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked',
    'Over18', 'OverTime', 'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction',
    'StandardHours', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance',
    'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager'
]

input_values = [34, 2, 628, 1, 8, 3, 3, 1, 2068, 2, 1, 82, 4, 2, 2, 3, 1,
                4404, 10228, 2, 0, 0, 12, 3, 1, 80, 0, 6, 3, 4, 4, 3, 1, 2]


input_df = pd.DataFrame([input_values], columns=columns)

# Predict attrition (0=No, 1=Yes)
prediction = model.predict(input_df)[0]
probability = model.predict_proba(input_df)[0][1]  # probability for class 1 (attrition)

print(f"Predicted Attrition: {prediction} (0=No, 1=Yes)")
print(f"Probability of Attrition: {probability:.2f}")

import pandas as pd
import joblib

model = joblib.load("best_attrition_model.pkl")
feature_cols = [
    'Age', 'BusinessTravel', 'DailyRate', 'Department', 'DistanceFromHome',
    'Education', 'EducationField', 'EmployeeCount', 'EmployeeNumber', 'EnvironmentSatisfaction',
    'Gender', 'HourlyRate', 'JobInvolvement', 'JobLevel', 'JobRole', 'JobSatisfaction',
    'MaritalStatus', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'Over18',
    'OverTime', 'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction',
    'StandardHours', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',
    'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion',
    'YearsWithCurrManager'
]

def predict_attrition(input_dict):
    input_df = pd.DataFrame([input_dict], columns=feature_cols)
    pred = model.predict(input_df)[0]
    pred_prob = model.predict_proba(input_df)[0][1]
    return pred, pred_prob

example_input = {
    'Age': 34,
    'BusinessTravel': 2,  #
    'DailyRate': 628,
    'Department': 1,
    'DistanceFromHome': 10,
    'Education': 3,
    'EducationField': 2,
    'EmployeeCount': 1,
    'EmployeeNumber': 1234,
    'EnvironmentSatisfaction': 3,
    'Gender': 1,
    'HourlyRate': 60,
    'JobInvolvement': 3,
    'JobLevel': 2,
    'JobRole': 5,
    'JobSatisfaction': 4,
    'MaritalStatus': 0,
    'MonthlyIncome': 6000,
    'MonthlyRate': 15000,
    'NumCompaniesWorked': 2,
    'Over18': 1,
    'OverTime': 0,
    'PercentSalaryHike': 12,
    'PerformanceRating': 3,
    'RelationshipSatisfaction': 2,
    'StandardHours': 80,
    'StockOptionLevel': 1,
    'TotalWorkingYears': 10,
    'TrainingTimesLastYear': 3,
    'WorkLifeBalance': 3,
    'YearsAtCompany': 5,
    'YearsInCurrentRole': 3,
    'YearsSinceLastPromotion': 1,
    'YearsWithCurrManager': 2
}

prediction, probability = predict_attrition(example_input)
print(f"Predicted Attrition: {prediction} (0=No, 1=Yes)")
print(f"Probability of Attrition: {probability:.2f}")

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

performance = {
    "DecisionTree": {"f1_class1": 0.5049, "recall_class1": 0.5532, "precision_class1": 0.4643, "accuracy": 0.8265},
    "LogisticRegression": {"f1_class1": 0.5035, "recall_class1": 0.7660, "precision_class1": 0.3750, "accuracy": 0.7585},
    "XGBoost": {"f1_class1": 0.4471, "recall_class1": 0.4043, "precision_class1": 0.5000, "accuracy": 0.8401},
    "RandomForest": {"f1_class1": 0.4211, "recall_class1": 0.3404, "precision_class1": 0.5517, "accuracy": 0.8503}
}

df_perf = pd.DataFrame(performance).T

plt.figure(figsize=(8, 5))
sns.barplot(x=df_perf.index, y='f1_class1', data=df_perf, palette='coolwarm')
plt.title('F1-Score for Class 1 (Attrition)')
plt.ylabel('F1-Score')
plt.ylim(0, 1)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
df_perf[['recall_class1', 'precision_class1']].plot(
    kind='bar',
    color=['#1f77b4', '#ff7f0e'],
    edgecolor='black'
)
plt.title('Recall and Precision for Class 1 (Attrition)')
plt.ylabel('Score')
plt.xticks(rotation=45)
plt.ylim(0, 1)
plt.legend(["Recall", "Precision"])
plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 5))
sns.barplot(x=df_perf.index, y='accuracy', data=df_perf, palette='Spectral')
plt.title('Overall Accuracy')
plt.ylabel('Accuracy')
plt.ylim(0, 1)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 5))
sns.lineplot(data=df_perf['precision_class1'], marker='o', linewidth=2)
plt.title('Precision for Class 1 (Attrition) Across Models')
plt.ylabel('Precision')
plt.xlabel('Model')
plt.ylim(0, 1)
plt.xticks(ticks=range(len(df_perf.index)), labels=df_perf.index, rotation=45)
plt.grid(True)
plt.tight_layout()
plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

conf_matrices = {
    "LogisticRegression": [[187, 60], [11, 36]],
    "DecisionTree": [[217, 30], [21, 26]],
    "RandomForest": [[234, 13], [31, 16]],
    "XGBoost": [[228, 19], [28, 19]]
}

fig, axes = plt.subplots(2, 2, figsize=(14, 12))
axes = axes.flatten()

for idx, (model_name, cm) in enumerate(conf_matrices.items()):
    sns.heatmap(
        cm, annot=True, fmt="d", cmap="Blues", cbar=False,
        xticklabels=["Predicted 0", "Predicted 1"],
        yticklabels=["Actual 0", "Actual 1"],
        ax=axes[idx], linewidths=0.5, linecolor='gray', square=True
    )
    axes[idx].set_title(f"{model_name} Confusion Matrix", fontsize=14, fontweight='bold')
    axes[idx].set_xlabel("Predicted Label", fontsize=12)
    axes[idx].set_ylabel("True Label", fontsize=12)

plt.suptitle("Confusion Matrices for All Models", fontsize=18, fontweight='bold')
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

import matplotlib.pyplot as plt

precision_scores = {
    "LogisticRegression": 0.3750,
    "DecisionTree": 0.4643,
    "RandomForest": 0.5517,
    "XGBoost": 0.5000
}


models = list(precision_scores.keys())
precision_values = list(precision_scores.values())


plt.figure(figsize=(8, 5))
plt.plot(models, precision_values, marker='o', linestyle='-', color='teal', linewidth=2)

for i, val in enumerate(precision_values):
    plt.text(i, val + 0.03, f"{val:.2f}", ha='center', va='bottom', fontsize=10, color='black')

plt.title("Precision for Class 1 (Attrition) Across Models", fontsize=14, fontweight='bold')
plt.xlabel("Model", fontsize=12)
plt.ylabel("Precision (Class 1)", fontsize=12)
plt.ylim(0, 1)
plt.grid(True, linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

import shap
import matplotlib.pyplot as plt
import joblib
import pandas as pd

model = joblib.load("best_attrition_model.pkl")
scaler = model.named_steps['scaler']
X_scaled = scaler.transform(X_train)

logistic_model = model.named_steps['model']

explainer = shap.LinearExplainer(logistic_model, X_scaled, feature_perturbation="interventional")
shap_values = explainer.shap_values(X_scaled)

shap.summary_plot(shap_values, X_scaled, plot_type="bar", show=False)
plt.title("SHAP Feature Importance (Logistic Regression - Class 1)")
plt.tight_layout()
plt.savefig("shap_logreg_summary.png", dpi=300)
plt.clf()

shap.summary_plot(shap_values, X_scaled, show=False)
plt.tight_layout()
plt.savefig("shap_logreg_beeswarm.png", dpi=300)
plt.clf()

idx = 5
shap.force_plot(
    explainer.expected_value,
    shap_values[idx, :],
    X_scaled[idx, :],
    matplotlib=True,
    show=False
)
plt.savefig("shap_logreg_force.png", dpi=300)
plt.clf()

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8,5))
sns.countplot(data=data, x='WorkLifeBalance', hue='Attrition')
plt.title('Attrition Count by WorkLifeBalance')
plt.show()

fig, axes = plt.subplots(1, 3, figsize=(18,5))

sns.histplot(data=data, x='YearsInCurrentRole', hue='Attrition', multiple='stack', ax=axes[0])
axes[0].set_title('Years In Current Role by Attrition')

sns.histplot(data=data, x='YearsSinceLastPromotion', hue='Attrition', multiple='stack', ax=axes[1])
axes[1].set_title('Years Since Last Promotion by Attrition')

sns.histplot(data=data, x='JobLevel', hue='Attrition', multiple='stack', ax=axes[2])
axes[2].set_title('Job Level by Attrition')

plt.tight_layout()
plt.show()

fig, axes = plt.subplots(1, 3, figsize=(18,5))
sns.countplot(data=data, x='Age', hue='Attrition', ax=axes[0])
axes[0].set_title('Attrition Count by Age')

sns.countplot(data=data, x='Gender', hue='Attrition', ax=axes[1])
axes[1].set_title('Attrition Count by Gender')

sns.countplot(data=data, x='MaritalStatus', hue='Attrition', ax=axes[2])
axes[2].set_title('Attrition Count by Marital Status')

plt.tight_layout()
plt.show()

